{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from layers.spectral_conv import SpectralConv2dLocalized\n",
    "from layers.scalq_eq_nonlin import scaleEqNonlin, scaleEqNonlinMaxp,scaleEqNonlin1d\n",
    "from torchvision.datasets import KMNIST,MNIST\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from utils.core_utils import resample\n",
    "import torch.functional as F\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "plt.rcParams[\"figure.figsize\"] = (4,4)\n",
    "color_theme = 'Blues'\n",
    "plt.rcParams.update({\n",
    "    'font.size': 12,\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"loading_image\"></a>\n",
    "### Load some random image "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_data = MNIST('./data',\\\n",
    "                    transform=torchvision.transforms.Compose([torchvision.transforms.ToTensor(),\\\n",
    "                                torchvision.transforms.Normalize((0.1307,), (0.3081,))]), download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img,l = mnist_data.__getitem__(7)\n",
    "plt.imshow(img[0], cmap=color_theme)\n",
    "plt.xlabel(str(l))\n",
    "plt.colorbar(shrink=0.8)\n",
    "plt.show()\n",
    "img = img.to(torch.float64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We denote input image by $I$. And by $f*I$, we will denote the convolution of image $I$ with some filter $f$. And we will denote the scaling operation by $G_s[I]$. In our case, the scaling operation $G_s$ is ideal resampling operation.\n",
    "\n",
    "Now, any operation, $P$, on the image $I$ will ve scale equivarinat if it commutes with the scaling operation $G_s$, i.e. $$ G_s[P(I)] = P(G_s[I])$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scale Equivarinat Convolution <a class=\"anchor\" id=\"conv_layer\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To attain scale-equivariant convolution, we execute the operation in the Fourier domain. While understanding local features is crutial in computer vision, spectral convolution functions globally. To capture local features effectively, we present a localized Fourier filter, imposing constrain in Spectral domain which ensures appropriate localization of the spatial kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "local modes: Active size of the Filter (Filter is 0 outside this size)\n",
    "global_modes: Global size of the Filter (Max size of Training Images)\n",
    "'''\n",
    "conv = SpectralConv2dLocalized(in_channel=1, out_channel=1,global_modes=28,local_modes=3)\n",
    "filter = conv.get_filters()\n",
    "filter_fourier_coefficients = conv.get_filters_spectral()\n",
    "s_filter = conv.get_filters_spectral()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize=(9, 4))\n",
    "\n",
    "axs[0].imshow(filter[0, 0].real.detach(), cmap=color_theme)\n",
    "axs[0].set_title('Spatially Localized Filter')\n",
    "axs[0].set_xlabel('X-axis')\n",
    "axs[0].set_ylabel('Y-axis')\n",
    "\n",
    "axs[1].imshow(torch.fft.fftshift(torch.abs(s_filter[0, 0]).detach()), cmap=color_theme)\n",
    "axs[1].set_title('Magnitude of Spectral Coefficients')\n",
    "axs[1].set_xlabel('Horizontal Frequency')\n",
    "axs[1].set_ylabel('Verticle Frequency')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "        conv_out = torch.fft.ifft2(conv(torch.fft.fft2(img[None,], norm='forward')), norm='forward').real\n",
    "        img_scaled = resample(img[None,], (16,16),complex=False, skip_nyq=True)[0]\n",
    "        conv_out_scaled = torch.fft.ifft2(conv(torch.fft.fft2(img_scaled[None], norm='forward')), norm='forward').real\n",
    "        scaled_conv_out = resample(conv_out, (16,16), complex=False, skip_nyq=True)\n",
    "        diff = scaled_conv_out - conv_out_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ploting routine\n",
    "fig, axs = plt.subplots(2, 3, figsize=(16, 10))\n",
    "img_plot = axs[0, 0].imshow(img[0], cmap=color_theme)\n",
    "plt.colorbar(img_plot, ax=axs[0, 0], shrink=0.85)\n",
    "axs[0, 0].set_title(\"Original Image: $I$\")\n",
    "conv_plot = axs[0, 1].imshow(conv_out[0, 0].detach(), cmap=color_theme)\n",
    "plt.colorbar(conv_plot, ax=axs[0, 1], shrink=0.85)\n",
    "axs[0, 1].set_title(\"Convolved Image: $f*I$\")\n",
    "scon_plot= axs[0, 2].imshow(scaled_conv_out[0,0], cmap=color_theme)\n",
    "plt.colorbar(scon_plot, ax=axs[0, 2], shrink=0.85)\n",
    "axs[0, 2].set_title(\"Scaled Convolved Image: $G_s[f *I]$\")\n",
    "simg_plot = axs[1, 0].imshow(img_scaled[0].detach(), cmap=color_theme)\n",
    "plt.colorbar(simg_plot, ax=axs[1, 0], shrink=0.85)\n",
    "axs[1, 0].set_title(\"Scaled Image: $G_s[I]$\")\n",
    "conv_scaled_plot = axs[1, 1].imshow(conv_out_scaled[0,0], cmap=color_theme)\n",
    "plt.colorbar(conv_scaled_plot, ax=axs[1, 1], shrink=0.85)\n",
    "axs[1, 1].set_title(\"Convolved Scaled Image: f*G_s(I)\")\n",
    "diff_plot = axs[1, 2].imshow(diff[0, 0].detach(), cmap=color_theme)\n",
    "axs[1, 2].set_title(\"Error: G_s[f*I] - f*G_s[I]\")\n",
    "plt.colorbar(diff_plot, ax=axs[1, 2], shrink=0.85)\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "plt.show()\n",
    "print(\"Equivariant Error:\", torch.norm(diff).item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that with our spacially local spectral convolution, the equivariance error is close to 0.\n",
    "\n",
    "On the other hand, if we repeat the same process regular $3\\times3$ convolution, we will find that, the operation is not Scale Equivarinat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regular_conv = torch.nn.Conv2d(in_channels=1, out_channels=1, kernel_size=3, stride=1, padding=1).to(img.dtype)\n",
    "\n",
    "with torch.no_grad():\n",
    "        conv_out = regular_conv(img[None,])\n",
    "        img_scaled = resample(img[None,], (16,16),complex=False, skip_nyq=True)[0]\n",
    "        conv_out_scaled = regular_conv(img_scaled[None])\n",
    "        scaled_conv_out = resample(conv_out, (16,16), complex=False, skip_nyq=True)\n",
    "        diff = scaled_conv_out - conv_out_scaled\n",
    "        \n",
    "fig, axs = plt.subplots(2, 3, figsize=(16, 10))\n",
    "img_plot = axs[0, 0].imshow(img[0], cmap=color_theme)\n",
    "plt.colorbar(img_plot, ax=axs[0, 0], shrink=0.85)\n",
    "axs[0, 0].set_title(\"Original Image: $I$\")\n",
    "conv_plot = axs[0, 1].imshow(conv_out[0, 0].detach(), cmap=color_theme)\n",
    "plt.colorbar(conv_plot, ax=axs[0, 1], shrink=0.85)\n",
    "axs[0, 1].set_title(\"Convolved Image: $f*I$\")\n",
    "scon_plot= axs[0, 2].imshow(scaled_conv_out[0,0], cmap=color_theme)\n",
    "plt.colorbar(scon_plot, ax=axs[0, 2], shrink=0.85)\n",
    "axs[0, 2].set_title(\"Scaled Convolved Image: $G_s[f *I]$\")\n",
    "simg_plot = axs[1, 0].imshow(img_scaled[0].detach(), cmap=color_theme)\n",
    "plt.colorbar(simg_plot, ax=axs[1, 0], shrink=0.85)\n",
    "axs[1, 0].set_title(\"Scaled Image: $G_s[I]$\")\n",
    "conv_scaled_plot = axs[1, 1].imshow(conv_out_scaled[0,0], cmap=color_theme)\n",
    "plt.colorbar(conv_scaled_plot, ax=axs[1, 1], shrink=0.85)\n",
    "axs[1, 1].set_title(\"Convolved Scaled Image: f*G_s(I)\")\n",
    "diff_plot = axs[1, 2].imshow(diff[0, 0].detach(), cmap=color_theme)\n",
    "axs[1, 2].set_title(\"Error: G_s[f*I] - f*G_s[I]\")\n",
    "plt.colorbar(diff_plot, ax=axs[1, 2], shrink=0.85)\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "plt.show()\n",
    "print(\"Equivariant Error:\", torch.norm(diff).item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scale Equivarinat Non-Linearity <a class=\"anchor\" id=\"non_linearity\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets test the propsoed scale equivarinat non-linearity. Even though we are using ReLu, we can pass any activation function of our choice to the ```scaleEqNonlin``` class and can obtain a scale-equivarinat version of that non-linear activation.\n",
    "\n",
    "Here, we will demosntrate Scale Equivarinat sigmoid function denated as $\\sigma_e$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "max_res: Max scale of the image in consideration ( For example max resolution in dataset)\n",
    "base_res: Lowest scale of the image in consideration\n",
    "increment: resolutions (and corrsponding scales) to skip. increment = 1 means no scales will be skipped.\n",
    "increment = 2, will skip every other scale. Layers will not be equivarinat to those scales.\n",
    "'''\n",
    "\n",
    "non_lin_of_choice = torch.sigmoid # any non-linearity will work\n",
    "Sc_eq_activation = scaleEqNonlin(non_lin_of_choice, base_res=8, normalization=None,\\\n",
    "                                 max_res=28, increment=1,channels=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    non_lin = torch.fft.ifft2(Sc_eq_activation(torch.fft.fft2(img[None,...],norm='forward')),norm='forward').real\n",
    "    img_scaled = resample(img[None,], (16,16),complex=False, skip_nyq=True)[0]\n",
    "    non_lin_scaled = torch.fft.ifft2(Sc_eq_activation(torch.fft.fft2(img_scaled[None],norm='forward')),norm='forward').real\n",
    "    #excluding the Nyquest Frequency as FFT can not determine it's complex part for real data\n",
    "    non_lin_scaled = resample(non_lin_scaled, (16,16),complex=False, skip_nyq=True)\n",
    "    \n",
    "    scaled_non_lin = resample(non_lin, (16,16),complex=False, skip_nyq=True)\n",
    "    diff = scaled_non_lin - non_lin_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, 2, figsize=(12, 8))\n",
    "axs[0, 0].imshow(non_lin[0, 0].detach().real, cmap=color_theme)\n",
    "axs[0, 0].set_title(\"$\\sigma_e(I)$\")\n",
    "plt.colorbar(axs[0, 0].imshow(non_lin[0, 0].detach().real, cmap=color_theme), ax=axs[0, 0])\n",
    "axs[0, 1].imshow(non_lin_scaled[0, 0].detach().real, cmap=color_theme)\n",
    "axs[0, 1].set_title(\"$\\sigma_e(G_s[I])$\")\n",
    "plt.colorbar(axs[0, 1].imshow(non_lin_scaled[0, 0].detach().real, cmap=color_theme), ax=axs[0, 1])\n",
    "axs[1, 0].imshow(scaled_non_lin[0, 0].detach().real, cmap=color_theme)\n",
    "axs[1, 0].set_title(\"$G_s[\\sigma_e(I)]$\")\n",
    "plt.colorbar(axs[1, 0].imshow(scaled_non_lin[0, 0].detach().real, cmap=color_theme), ax=axs[1, 0])\n",
    "axs[1, 1].imshow(diff[0, 0].detach().real, cmap=color_theme)\n",
    "axs[1, 1].set_title(\"Error: $G_s[\\sigma_e(I)]-\\sigma_e(G_s[I])$\")\n",
    "plt.colorbar(axs[1, 1].imshow(diff[0, 0].detach().real, cmap=color_theme), ax=axs[1, 1])\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print(\"Equivariant Error:\", torch.norm(diff).item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, lets observe the situation with regular Sigmoid function applied pointwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_lin = non_lin_of_choice(img[None,...])\n",
    "img_scaled = resample(img[None,], (16,16),complex=False, skip_nyq=True)[0]\n",
    "non_lin_scaled = non_lin_of_choice(img_scaled[None])\n",
    "#excluding the Nyquest Frequency as FFT can not determine it's complex part for real data\n",
    "non_lin_scaled = resample(non_lin_scaled, (16,16),complex=False, skip_nyq=True)    \n",
    "scaled_non_lin = resample(non_lin, (16,16),complex=False, skip_nyq=True)\n",
    "diff = scaled_non_lin - non_lin_scaled\n",
    "\n",
    "fig, axs = plt.subplots(2, 2, figsize=(12, 8))\n",
    "axs[0, 0].imshow(non_lin[0, 0].detach().real, cmap=color_theme)\n",
    "axs[0, 0].set_title(\"$\\sigma(I)$\")\n",
    "plt.colorbar(axs[0, 0].imshow(non_lin[0, 0].detach().real, cmap=color_theme), ax=axs[0, 0])\n",
    "axs[0, 1].imshow(non_lin_scaled[0, 0].detach().real, cmap=color_theme)\n",
    "axs[0, 1].set_title(\"$\\sigma(G[I])$\")\n",
    "plt.colorbar(axs[0, 1].imshow(non_lin_scaled[0, 0].detach().real, cmap=color_theme), ax=axs[0, 1])\n",
    "axs[1, 0].imshow(scaled_non_lin[0, 0].detach().real, cmap=color_theme)\n",
    "axs[1, 0].set_title(\"$G_s[\\sigma(I)]$\")\n",
    "plt.colorbar(axs[1, 0].imshow(scaled_non_lin[0, 0].detach().real, cmap=color_theme), ax=axs[1, 0])\n",
    "axs[1, 1].imshow(diff[0, 0].detach().real, cmap=color_theme)\n",
    "axs[1, 1].set_title(\"Error: $G_s[\\sigma(I)]-\\sigma(G_s[I])$\")\n",
    "plt.colorbar(axs[1, 1].imshow(diff[0, 0].detach().real, cmap=color_theme), ax=axs[1, 1])\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print(\"Equivariant Error:\", torch.norm(diff).item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scale Equivarinat CNNs <a class=\"anchor\" id=\"CNN\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will create a multi-layered CNN with scale equivarinat layers. And we will show the whole achitecture is Scale Equivariant.\n",
    "\n",
    "We denote the features extracted by the CNN  as $M(I)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaleEqModel(nn.Module):\n",
    "    def __init__(self,):\n",
    "        super(ScaleEqModel, self).__init__()\n",
    "        self.conv1 = SpectralConv2dLocalized(in_channel=1, out_channel=16,\\\n",
    "                                             global_modes=28,local_modes=5)\n",
    "        self.non_linear1 = scaleEqNonlinMaxp(nn.functional.sigmoid, 8,\\\n",
    "                                             normalization = None, channels = 16,\\\n",
    "                                             pool_window = 2, max_res = 28, increment = 1)\n",
    "\n",
    "        # note that for the following layers, we use the global_modes = 14, which half of the previous layers\n",
    "        # this is because of the max_pool by window size of 2, which scales down by 2. (28 -> 14). \n",
    "        # Same Goes for the base base_res(8->4)\n",
    "        self.conv2 = SpectralConv2dLocalized(in_channel=16, out_channel=32,\\\n",
    "                                             global_modes=14,local_modes=5)\n",
    "\n",
    "        #Not doing max pool in the last layer\n",
    "        self.non_linear2 = scaleEqNonlin(nn.functional.sigmoid, 4,\\\n",
    "                                         normalization = None, channels = 32,\\\n",
    "                                         max_res = 14, increment = 1)\n",
    "        \n",
    "        self.linear = nn.Linear(32*14*14, 10)\n",
    "    \n",
    "    def get_feature(self, x):\n",
    "        x_ft = torch.fft.fft2(x, norm = 'forward')\n",
    "        c1 = self.conv1(x_ft)\n",
    "        n1 = self.non_linear1(c1)\n",
    "        c2 = self.conv2(n1)\n",
    "        n2 = self.non_linear2(c2)\n",
    "        features = torch.fft.ifft2(n2, norm = 'forward').real\n",
    "        return features\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.get_feature(x)\n",
    "        # reshape features to a fixed size\n",
    "        features_fixed_size = resample(features, (14,14))\n",
    "        logits = self.linear(features_fixed_size.reshape(features_fixed_size.shape[0], -1))\n",
    "        return logits.real\n",
    "\n",
    "model = ScaleEqModel().to(torch.cdouble)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here the final features of out mode will be down-scaled by a factor of 2 due to the max-pooling operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    model_out = model.get_feature(img[None,...]) # output shape (..., 14x14)\n",
    "    \n",
    "    img_scaled = resample(img[None,], (16,16),\\\n",
    "                          complex=False, skip_nyq=True)[0] # down-scaled image size 16x16 \n",
    "    \n",
    "    model_out_scaled = model.get_feature(img_scaled[None]) #output size 8x8\n",
    "\n",
    "    #removing the Nyquest\n",
    "    model_out_scaled = resample(model_out_scaled, (8,8),complex=False, skip_nyq=True)\n",
    "    \n",
    "    scaled_model_out = resample(model_out, (8,8),complex=False, skip_nyq=True)\n",
    "    diff = scaled_model_out - model_out_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, 2, figsize=(12, 10))\n",
    "axs[0, 0].set_title(\"Features: $M(I)$\")\n",
    "plt.colorbar(axs[0, 0].imshow(model_out[0, 0].detach().real, cmap=color_theme), ax=axs[0, 0])\n",
    "\n",
    "axs[0, 1].set_title(\"Features from Scaled Image: $M(G_s[I])$\")\n",
    "plt.colorbar(axs[0, 1].imshow(model_out_scaled[0, 0].detach().real, cmap=color_theme), ax=axs[0, 1])\n",
    "\n",
    "axs[1, 0].set_title(\"Scaled Features: $G_s[M(I)]$\")\n",
    "plt.colorbar(axs[1, 0].imshow(scaled_model_out[0, 0].detach().real, cmap=color_theme), ax=axs[1, 0])\n",
    "\n",
    "axs[1, 1].imshow(diff[0, 0].detach().real, cmap=color_theme)\n",
    "axs[1, 1].set_title(\"Error: $G_s[M(I)]-M(G_s[I])$\")\n",
    "plt.colorbar(axs[1, 1].imshow(diff[0, 0].detach().real, cmap=color_theme), ax=axs[1, 1])\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print(\"Equivariant Error:\", torch.norm(diff).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model on MNIST\n",
    "train_loader = torch.utils.data.DataLoader(mnist_data, batch_size=1024, shuffle=True)\n",
    "import torch.optim as optim\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "import time\n",
    "start_time = time.time()\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "model = model.cuda()\n",
    "model.train()\n",
    "for epoch in range(5):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data = data.to(torch.cdouble).cuda()\n",
    "        target = target.cuda()\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data.to(torch.cdouble))\n",
    "        loss = loss_func(output, target.to(torch.long))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1}, Loss: {running_loss / len(train_loader)}\")\n",
    "end_time = time.time()\n",
    "print(\"Training Time: \", end_time - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 400x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "imge_array = []\n",
    "feature_array_0 = []\n",
    "feature_array_1 = []\n",
    "feature_array_2 = []\n",
    "\n",
    "# get an image from MNIST dataset\n",
    "# 32, 22, 15\n",
    "img, l = mnist_data.__getitem__(15)\n",
    "img = img.to(torch.float64)\n",
    "plt.imshow(img[0], cmap=color_theme)\n",
    "plt.show()\n",
    "\n",
    "img = img.cuda()\n",
    "for i in range(8, 64, 1):\n",
    "    img_scaled = resample(img[None,], (i,i),\\\n",
    "                          complex=False, skip_nyq=True)[0] # down-scaled image size 16x16 \n",
    "    \n",
    "    model_out_scaled = model.get_feature(img_scaled[None]) #output size 8x8\n",
    "\n",
    "    imge_array.append(img_scaled[0].detach().cpu().numpy())\n",
    "    feature_array_0.append(model_out_scaled[0, 0].detach().cpu().numpy())\n",
    "    feature_array_1.append(model_out_scaled[0, 1].detach().cpu().numpy())\n",
    "    feature_array_2.append(model_out_scaled[0, 2].detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making Gif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reverse the list\n",
    "imge_array.reverse()\n",
    "feature_array_0.reverse()\n",
    "feature_array_1.reverse()\n",
    "feature_array_2.reverse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_706402/606058520.py:40: DeprecationWarning: Starting with ImageIO v3 the behavior of this function will switch to that of iio.v3.imread. To keep the current behavior (and make this warning disappear) use `import imageio.v2 as imageio` or call `imageio.v2.imread` directly.\n",
      "  image = imageio.imread(filename)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GIF saved as image_feature_animation.gif\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import imageio\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Create a folder to store individual frames\n",
    "os.makedirs(\"frames\", exist_ok=True)\n",
    "\n",
    "filenames = []\n",
    "\n",
    "for i, (img_, feat_0, feat_1, feat_2) in enumerate(zip(imge_array, feature_array_0, feature_array_1, feature_array_2)):\n",
    "    fig, axs = plt.subplots(1, 4, figsize=(12, 3))\n",
    "\n",
    "    axs[0].imshow(img_, cmap='gray')\n",
    "    axs[0].set_title(\"Input Image - Resolution: {}\".format(img_.shape[-2:]))\n",
    "    axs[0].axis('off')\n",
    "\n",
    "    axs[1].imshow(feat_0, cmap='viridis')\n",
    "    axs[1].set_title(\"Feature Map 0\")\n",
    "    axs[1].axis('off')\n",
    "    \n",
    "    axs[2].imshow(feat_1, cmap='viridis')\n",
    "    axs[2].set_title(\"Feature Map 1\")\n",
    "    axs[2].axis('off')\n",
    "    \n",
    "    axs[3].imshow(feat_2, cmap='viridis')\n",
    "    axs[3].set_title(\"Feature Map 2\")\n",
    "    axs[3].axis('off')\n",
    "\n",
    "    fname = f\"frames/frame_{i:02d}.png\"\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(fname)\n",
    "    plt.close(fig)\n",
    "    filenames.append(fname)\n",
    "\n",
    "# Create GIF\n",
    "gif_path = \"image_feature_animation.gif\"\n",
    "with imageio.get_writer(gif_path, mode='I', duration=0.5) as writer:\n",
    "    for filename in filenames:\n",
    "        image = imageio.imread(filename)\n",
    "        writer.append_data(image)\n",
    "\n",
    "print(f\"GIF saved as {gif_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scale Equivarinat Linear Layer <a class=\"anchor\" id=\"linear_layer\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will explore scale equivarinat linear layer. Given a singnal $f$ of lenght $N$, a linear operation on the signal is represented as $f_{out} = W \\times f$, where $W$ is $N\\times N$ matrix.\n",
    "\n",
    "In general this operation is not scale equivarinat. But following our claim on scale equivarinat operation, we can construct scale Equivarinat Linear Layer. We demote the linear layer as $L$.\n",
    "\n",
    "We have two modules ```SpectralMixer_1D``` ,and ```SpectralMixer_2D``` for signal on 1D and 2D domain respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchaudio\n",
    "from utils.core_utils import resample_1d \n",
    "plt.rcParams[\"figure.figsize\"] = (30,7)\n",
    "plt.rcParams.update({\n",
    "    'font.size': 16,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yesno_data = torchaudio.datasets.YESNO('./data', download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = yesno_data.__getitem__(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For visualization we will reduce the length (time duration) of the audio signals. Here, we denote the audio signal as $f$ and its' Fourier Transform a $F$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = p[0][0,6500:7000].to(torch.double)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot([i/8000 for i in range(f.shape[-1])],f, marker='o', markersize=4, linestyle='-', color='b')\n",
    "plt.xlabel('Time (s)')\n",
    "plt.ylabel('Amplitude')\n",
    "plt.title('Original Signal: $f$')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F = torch.fft.fftshift(torch.fft.fft(f))\n",
    "plt.bar([i-F.shape[-1]//2 for i in range(F.shape[-1])],torch.abs(F), color='green', width=1.0)\n",
    "plt.xlabel('Frequency')\n",
    "plt.ylabel('Amplitude')\n",
    "plt.title('Fourier Transform Magnitude: $|F|$')\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from layers.spectral_mixer import SpectralMixer_1D, SpectralMixer_2D\n",
    "'''\n",
    "num_channels: Number of Co-domain of the input signal\n",
    "mode_size: Maximum number of Fourier models to use. Normally can be set to size of the input signal.\n",
    "mixer_band: Limits the number of neighboring Fourier models to mix with each other. Deafults to -1 implies no limit on mixing.\n",
    "'''\n",
    "s_eq_linear = SpectralMixer_1D( num_channels=1, mode_size=500, mixer_band=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    f_out = torch.fft.ifft(s_eq_linear(torch.fft.fft(f[None,None,...],\\\n",
    "                                                     norm='forward'), 500), norm='forward')   \n",
    "    f_scaled = resample_1d(f[None,None,...], 250, skip_nyq=True)\n",
    "    f_scaled_out = torch.fft.ifft(s_eq_linear(torch.fft.fft(f_scaled,\\\n",
    "                                                            norm='forward'), 250), norm='forward')\n",
    "    f_scaled_out = resample_1d(f_scaled_out, 250, skip_nyq=True)\n",
    "    scaled_f_out =  resample_1d(f_out, 250, skip_nyq=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_values = [i / 8000 for i in range(f_out.shape[-1])]\n",
    "plt.plot(time_values, f_out[0, 0].real, marker='o', markersize=4, linestyle='-', color='b')\n",
    "plt.xlabel('Time (s)')\n",
    "plt.ylabel('Amplitude')\n",
    "plt.title('Transformed Signal: $L(f)$')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Compute f_scaled\n",
    "time_values = [i / 4000 for i in range(f_scaled.shape[-1])]\n",
    "plt.plot(time_values, f_scaled[0, 0].real, marker='o', markersize=4, linestyle='-', color='g')\n",
    "plt.xlabel('Time (s)')\n",
    "plt.ylabel('Amplitude')\n",
    "plt.title('Scaled Input Signal: $G_s(f)$')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Compute f_scaled_out\n",
    "time_values = [i / 4000 for i in range(f_scaled_out.shape[-1])]\n",
    "plt.plot(time_values, f_scaled_out[0, 0].real, marker='o', markersize=4, linestyle='-', color='r')\n",
    "plt.xlabel('Time (s)')\n",
    "plt.ylabel('Amplitude')\n",
    "plt.title('Transformed Scaled Signal: $L(G_s[f])$')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Compute difference signal\n",
    "diff = (f_scaled_out[0, 0] - scaled_f_out[0, 0]).real\n",
    "time_values = [i / 4000 for i in range(diff.shape[-1])]\n",
    "plt.plot(time_values, diff, marker='o', markersize=4, linestyle='-', color='orange')\n",
    "plt.xlabel('Time (s)')\n",
    "plt.ylabel('Amplitude')\n",
    "plt.title('Equivariance Error: $L(G_s[f]) - G_s[L(f)]$')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "print(\"Equivariant Error:\", torch.norm(diff).item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Together with the this scale equivarinat linear operation and activtion ```scaleEqNonlin1d```, we can also construct Fully connected Scale Equivariant network. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neuralop",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
