{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scale_eq.layers.spectral_conv import SpectralConv2dLocalized\n",
    "from scale_eq.layers.scalq_eq_nonlin import scaleEqNonlin, scaleEqNonlinMaxp,scaleEqNonlin1d\n",
    "from torchvision.datasets import KMNIST,MNIST\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from scale_eq.utils.core_utils import resample\n",
    "import torch.functional as F\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "plt.rcParams[\"figure.figsize\"] = (4,4)\n",
    "color_theme = 'Blues'\n",
    "plt.rcParams.update({\n",
    "    'font.size': 12,\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"loading_image\"></a>\n",
    "### Load some random image "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_data = MNIST('./data',\\\n",
    "                    transform=torchvision.transforms.Compose([torchvision.transforms.ToTensor(),\\\n",
    "                                torchvision.transforms.Normalize((0.1307,), (0.3081,))]), download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img,l = mnist_data.__getitem__(7)\n",
    "plt.imshow(img[0], cmap=color_theme)\n",
    "plt.xlabel(str(l))\n",
    "plt.colorbar(shrink=0.8)\n",
    "plt.show()\n",
    "img = img.to(torch.float64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We denote input image by $I$. And by $f*I$, we will denote the convolution of image $I$ with some filter $f$. And we will denote the scaling operation by $G_s[I]$. In our case, the scaling operation $G_s$ is ideal resampling operation.\n",
    "\n",
    "Now, any operation, $P$, on the image $I$ will ve scale equivarinat if it commutes with the scaling operation $G_s$, i.e. $$ G_s[P(I)] = P(G_s[I])$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scale Equivarinat Convolution <a class=\"anchor\" id=\"conv_layer\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To attain scale-equivariant convolution, we execute the operation in the Fourier domain. While understanding local features is crutial in computer vision, spectral convolution functions globally. To capture local features effectively, we present a localized Fourier filter, imposing constrain in Spectral domain which ensures appropriate localization of the spatial kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "local modes: Active size of the Filter (Filter is 0 outside this size)\n",
    "global_modes: Global size of the Filter (Max size of Training Images)\n",
    "'''\n",
    "conv = SpectralConv2dLocalized(in_channel=1, out_channel=1,global_modes=28,local_modes=3)\n",
    "filter = conv.get_filters()\n",
    "filter_fourier_coefficients = conv.get_filters_spectral()\n",
    "s_filter = conv.get_filters_spectral()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize=(9, 4))\n",
    "\n",
    "axs[0].imshow(filter[0, 0].real.detach(), cmap=color_theme)\n",
    "axs[0].set_title('Spatially Localized Filter')\n",
    "axs[0].set_xlabel('X-axis')\n",
    "axs[0].set_ylabel('Y-axis')\n",
    "\n",
    "axs[1].imshow(torch.fft.fftshift(torch.abs(s_filter[0, 0]).detach()), cmap=color_theme)\n",
    "axs[1].set_title('Magnitude of Spectral Coefficients')\n",
    "axs[1].set_xlabel('Horizontal Frequency')\n",
    "axs[1].set_ylabel('Verticle Frequency')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "        conv_out = torch.fft.ifft2(conv(torch.fft.fft2(img[None,], norm='forward')), norm='forward').real\n",
    "        img_scaled = resample(img[None,], (16,16),complex=False, skip_nyq=True)[0]\n",
    "        conv_out_scaled = torch.fft.ifft2(conv(torch.fft.fft2(img_scaled[None], norm='forward')), norm='forward').real\n",
    "        scaled_conv_out = resample(conv_out, (16,16), complex=False, skip_nyq=True)\n",
    "        diff = scaled_conv_out - conv_out_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ploting routine\n",
    "fig, axs = plt.subplots(2, 3, figsize=(16, 10))\n",
    "img_plot = axs[0, 0].imshow(img[0], cmap=color_theme)\n",
    "plt.colorbar(img_plot, ax=axs[0, 0], shrink=0.85)\n",
    "axs[0, 0].set_title(\"Original Image: $I$\")\n",
    "conv_plot = axs[0, 1].imshow(conv_out[0, 0].detach(), cmap=color_theme)\n",
    "plt.colorbar(conv_plot, ax=axs[0, 1], shrink=0.85)\n",
    "axs[0, 1].set_title(\"Convolved Image: $f*I$\")\n",
    "scon_plot= axs[0, 2].imshow(scaled_conv_out[0,0], cmap=color_theme)\n",
    "plt.colorbar(scon_plot, ax=axs[0, 2], shrink=0.85)\n",
    "axs[0, 2].set_title(\"Scaled Convolved Image: $G_s[f *I]$\")\n",
    "simg_plot = axs[1, 0].imshow(img_scaled[0].detach(), cmap=color_theme)\n",
    "plt.colorbar(simg_plot, ax=axs[1, 0], shrink=0.85)\n",
    "axs[1, 0].set_title(\"Scaled Image: $G_s[I]$\")\n",
    "conv_scaled_plot = axs[1, 1].imshow(conv_out_scaled[0,0], cmap=color_theme)\n",
    "plt.colorbar(conv_scaled_plot, ax=axs[1, 1], shrink=0.85)\n",
    "axs[1, 1].set_title(\"Convolved Scaled Image: f*G_s(I)\")\n",
    "diff_plot = axs[1, 2].imshow(diff[0, 0].detach(), cmap=color_theme)\n",
    "axs[1, 2].set_title(\"Error: G_s[f*I] - f*G_s[I]\")\n",
    "plt.colorbar(diff_plot, ax=axs[1, 2], shrink=0.85)\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "plt.show()\n",
    "print(\"Equivariant Error:\", torch.norm(diff).item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that with our spacially local spectral convolution, the equivariance error is close to 0.\n",
    "\n",
    "On the other hand, if we repeat the same process regular $3\\times3$ convolution, we will find that, the operation is not Scale Equivarinat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regular_conv = torch.nn.Conv2d(in_channels=1, out_channels=1, kernel_size=3, stride=1, padding=1).to(img.dtype)\n",
    "\n",
    "with torch.no_grad():\n",
    "        conv_out = regular_conv(img[None,])\n",
    "        img_scaled = resample(img[None,], (16,16),complex=False, skip_nyq=True)[0]\n",
    "        conv_out_scaled = regular_conv(img_scaled[None])\n",
    "        scaled_conv_out = resample(conv_out, (16,16), complex=False, skip_nyq=True)\n",
    "        diff = scaled_conv_out - conv_out_scaled\n",
    "        \n",
    "fig, axs = plt.subplots(2, 3, figsize=(16, 10))\n",
    "img_plot = axs[0, 0].imshow(img[0], cmap=color_theme)\n",
    "plt.colorbar(img_plot, ax=axs[0, 0], shrink=0.85)\n",
    "axs[0, 0].set_title(\"Original Image: $I$\")\n",
    "conv_plot = axs[0, 1].imshow(conv_out[0, 0].detach(), cmap=color_theme)\n",
    "plt.colorbar(conv_plot, ax=axs[0, 1], shrink=0.85)\n",
    "axs[0, 1].set_title(\"Convolved Image: $f*I$\")\n",
    "scon_plot= axs[0, 2].imshow(scaled_conv_out[0,0], cmap=color_theme)\n",
    "plt.colorbar(scon_plot, ax=axs[0, 2], shrink=0.85)\n",
    "axs[0, 2].set_title(\"Scaled Convolved Image: $G_s[f *I]$\")\n",
    "simg_plot = axs[1, 0].imshow(img_scaled[0].detach(), cmap=color_theme)\n",
    "plt.colorbar(simg_plot, ax=axs[1, 0], shrink=0.85)\n",
    "axs[1, 0].set_title(\"Scaled Image: $G_s[I]$\")\n",
    "conv_scaled_plot = axs[1, 1].imshow(conv_out_scaled[0,0], cmap=color_theme)\n",
    "plt.colorbar(conv_scaled_plot, ax=axs[1, 1], shrink=0.85)\n",
    "axs[1, 1].set_title(\"Convolved Scaled Image: f*G_s(I)\")\n",
    "diff_plot = axs[1, 2].imshow(diff[0, 0].detach(), cmap=color_theme)\n",
    "axs[1, 2].set_title(\"Error: G_s[f*I] - f*G_s[I]\")\n",
    "plt.colorbar(diff_plot, ax=axs[1, 2], shrink=0.85)\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "plt.show()\n",
    "print(\"Equivariant Error:\", torch.norm(diff).item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scale Equivarinat Non-Linearity <a class=\"anchor\" id=\"non_linearity\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets test the propsoed scale equivarinat non-linearity. Even though we are using ReLu, we can pass any activation function of our choice to the ```scaleEqNonlin``` class and can obtain a scale-equivarinat version of that non-linear activation.\n",
    "\n",
    "Here, we will demosntrate Scale Equivarinat sigmoid function denated as $\\sigma_e$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "max_res: Max scale of the image in consideration ( For example max resolution in dataset)\n",
    "base_res: Lowest scale of the image in consideration\n",
    "increment: resolutions (and corrsponding scales) to skip. increment = 1 means no scales will be skipped.\n",
    "increment = 2, will skip every other scale. Layers will not be equivarinat to those scales.\n",
    "'''\n",
    "\n",
    "non_lin_of_choice = torch.sigmoid # any non-linearity will work\n",
    "Sc_eq_activation = scaleEqNonlin(non_lin_of_choice, base_res=8, normalization=None,\\\n",
    "                                 max_res=28, increment=1,channels=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    non_lin = torch.fft.ifft2(Sc_eq_activation(torch.fft.fft2(img[None,...],norm='forward')),norm='forward').real\n",
    "    img_scaled = resample(img[None,], (16,16),complex=False, skip_nyq=True)[0]\n",
    "    non_lin_scaled = torch.fft.ifft2(Sc_eq_activation(torch.fft.fft2(img_scaled[None],norm='forward')),norm='forward').real\n",
    "    #excluding the Nyquest Frequency as FFT can not determine it's complex part for real data\n",
    "    non_lin_scaled = resample(non_lin_scaled, (16,16),complex=False, skip_nyq=True)\n",
    "    \n",
    "    scaled_non_lin = resample(non_lin, (16,16),complex=False, skip_nyq=True)\n",
    "    diff = scaled_non_lin - non_lin_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, 2, figsize=(12, 8))\n",
    "axs[0, 0].imshow(non_lin[0, 0].detach().real, cmap=color_theme)\n",
    "axs[0, 0].set_title(\"$\\sigma_e(I)$\")\n",
    "plt.colorbar(axs[0, 0].imshow(non_lin[0, 0].detach().real, cmap=color_theme), ax=axs[0, 0])\n",
    "axs[0, 1].imshow(non_lin_scaled[0, 0].detach().real, cmap=color_theme)\n",
    "axs[0, 1].set_title(\"$\\sigma_e(G_s[I])$\")\n",
    "plt.colorbar(axs[0, 1].imshow(non_lin_scaled[0, 0].detach().real, cmap=color_theme), ax=axs[0, 1])\n",
    "axs[1, 0].imshow(scaled_non_lin[0, 0].detach().real, cmap=color_theme)\n",
    "axs[1, 0].set_title(\"$G_s[\\sigma_e(I)]$\")\n",
    "plt.colorbar(axs[1, 0].imshow(scaled_non_lin[0, 0].detach().real, cmap=color_theme), ax=axs[1, 0])\n",
    "axs[1, 1].imshow(diff[0, 0].detach().real, cmap=color_theme)\n",
    "axs[1, 1].set_title(\"Error: $G_s[\\sigma_e(I)]-\\sigma_e(G_s[I])$\")\n",
    "plt.colorbar(axs[1, 1].imshow(diff[0, 0].detach().real, cmap=color_theme), ax=axs[1, 1])\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print(\"Equivariant Error:\", torch.norm(diff).item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, lets observe the situation with regular Sigmoid function applied pointwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_lin = non_lin_of_choice(img[None,...])\n",
    "img_scaled = resample(img[None,], (16,16),complex=False, skip_nyq=True)[0]\n",
    "non_lin_scaled = non_lin_of_choice(img_scaled[None])\n",
    "#excluding the Nyquest Frequency as FFT can not determine it's complex part for real data\n",
    "non_lin_scaled = resample(non_lin_scaled, (16,16),complex=False, skip_nyq=True)    \n",
    "scaled_non_lin = resample(non_lin, (16,16),complex=False, skip_nyq=True)\n",
    "diff = scaled_non_lin - non_lin_scaled\n",
    "\n",
    "fig, axs = plt.subplots(2, 2, figsize=(12, 8))\n",
    "axs[0, 0].imshow(non_lin[0, 0].detach().real, cmap=color_theme)\n",
    "axs[0, 0].set_title(\"$\\sigma(I)$\")\n",
    "plt.colorbar(axs[0, 0].imshow(non_lin[0, 0].detach().real, cmap=color_theme), ax=axs[0, 0])\n",
    "axs[0, 1].imshow(non_lin_scaled[0, 0].detach().real, cmap=color_theme)\n",
    "axs[0, 1].set_title(\"$\\sigma(G[I])$\")\n",
    "plt.colorbar(axs[0, 1].imshow(non_lin_scaled[0, 0].detach().real, cmap=color_theme), ax=axs[0, 1])\n",
    "axs[1, 0].imshow(scaled_non_lin[0, 0].detach().real, cmap=color_theme)\n",
    "axs[1, 0].set_title(\"$G_s[\\sigma(I)]$\")\n",
    "plt.colorbar(axs[1, 0].imshow(scaled_non_lin[0, 0].detach().real, cmap=color_theme), ax=axs[1, 0])\n",
    "axs[1, 1].imshow(diff[0, 0].detach().real, cmap=color_theme)\n",
    "axs[1, 1].set_title(\"Error: $G_s[\\sigma(I)]-\\sigma(G_s[I])$\")\n",
    "plt.colorbar(axs[1, 1].imshow(diff[0, 0].detach().real, cmap=color_theme), ax=axs[1, 1])\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print(\"Equivariant Error:\", torch.norm(diff).item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scale Equivarinat CNNs <a class=\"anchor\" id=\"CNN\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will create a multi-layered CNN with scale equivarinat layers. And we will show the whole achitecture is Scale Equivariant.\n",
    "\n",
    "We denote the features extracted by the CNN  as $M(I)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaleEqModel(nn.Module):\n",
    "    def __init__(self,\n",
    "                 scale_eq = True):\n",
    "        super(ScaleEqModel, self).__init__()\n",
    "        self.scale_eq = scale_eq\n",
    "        if scale_eq:\n",
    "            self.conv1 = SpectralConv2dLocalized(in_channel=1, out_channel=16,\\\n",
    "                                                global_modes=28,local_modes=5)\n",
    "            self.non_linear1 = scaleEqNonlinMaxp(nn.functional.sigmoid, 8,\\\n",
    "                                                normalization = None, channels = 16,\\\n",
    "                                                pool_window = 2, max_res = 28, increment = 1)\n",
    "\n",
    "            # note that for the following layers, we use the global_modes = 14, which half of the previous layers\n",
    "            # this is because of the max_pool by window size of 2, which scales down by 2. (28 -> 14). \n",
    "            # Same Goes for the base base_res(8->4)\n",
    "            self.conv2 = SpectralConv2dLocalized(in_channel=16, out_channel=32,\\\n",
    "                                                global_modes=14,local_modes=5)\n",
    "\n",
    "            #Not doing max pool in the last layer\n",
    "            self.non_linear2 = scaleEqNonlin(nn.functional.sigmoid, 4,\\\n",
    "                                            normalization = None, channels = 32,\\\n",
    "                                            max_res = 14, increment = 1)\n",
    "            \n",
    "            self.linear = nn.Linear(32*14*14, 10)\n",
    "        else:\n",
    "            self.conv1 = nn.Conv2d(1, 16, kernel_size=11, stride=1, padding=5)\n",
    "            self.non_linear1 = nn.Sequential(\n",
    "                nn.ReLU(),\n",
    "                nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "            )\n",
    "            self.conv2 = nn.Conv2d(16, 32, kernel_size=11, stride=1, padding=5)\n",
    "            self.non_linear2 = nn.ReLU()\n",
    "            self.linear = nn.Linear(32*14*14, 10)\n",
    "    \n",
    "    def get_feature(self, x):\n",
    "        if self.scale_eq:\n",
    "            x_ft = torch.fft.fft2(x, norm = 'forward')\n",
    "        else:\n",
    "            x_ft = x\n",
    "        c1 = self.conv1(x_ft)\n",
    "        n1 = self.non_linear1(c1)\n",
    "        c2 = self.conv2(n1)\n",
    "        n2 = self.non_linear2(c2)\n",
    "        if self.scale_eq:\n",
    "            features = torch.fft.ifft2(n2, norm = 'forward').real\n",
    "        else:\n",
    "            features = n2\n",
    "        return features\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.get_feature(x)\n",
    "        # reshape features to a fixed size\n",
    "        features_fixed_size = resample(features, (14,14))\n",
    "        if not self.scale_eq:\n",
    "            features_fixed_size = features_fixed_size.real\n",
    "        logits = self.linear(features_fixed_size.reshape(features_fixed_size.shape[0], -1))\n",
    "        return logits if not self.scale_eq else logits.real\n",
    "scale_eq = True\n",
    "dtype = torch.cdouble if scale_eq else torch.double\n",
    "model = ScaleEqModel(scale_eq=scale_eq).to(dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here the final features of out mode will be down-scaled by a factor of 2 due to the max-pooling operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = img.cpu().to(torch.double)\n",
    "with torch.no_grad():\n",
    "    model_out = model.get_feature(img[None,...]) # output shape (..., 14x14)\n",
    "    \n",
    "    img_scaled = resample(img[None,], (16,16),\\\n",
    "                          complex=False, skip_nyq=True)[0] # down-scaled image size 16x16 \n",
    "    \n",
    "    model_out_scaled = model.get_feature(img_scaled[None]) #output size 8x8\n",
    "\n",
    "    #removing the Nyquest\n",
    "    model_out_scaled = resample(model_out_scaled, (8,8),complex=False, skip_nyq=True)\n",
    "    \n",
    "    scaled_model_out = resample(model_out, (8,8),complex=False, skip_nyq=True)\n",
    "    diff = scaled_model_out - model_out_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, 2, figsize=(12, 10))\n",
    "axs[0, 0].set_title(\"Features: $M(I)$\")\n",
    "plt.colorbar(axs[0, 0].imshow(model_out[0, 0].detach().real, cmap=color_theme), ax=axs[0, 0])\n",
    "\n",
    "axs[0, 1].set_title(\"Features from Scaled Image: $M(G_s[I])$\")\n",
    "plt.colorbar(axs[0, 1].imshow(model_out_scaled[0, 0].detach().real, cmap=color_theme), ax=axs[0, 1])\n",
    "\n",
    "axs[1, 0].set_title(\"Scaled Features: $G_s[M(I)]$\")\n",
    "plt.colorbar(axs[1, 0].imshow(scaled_model_out[0, 0].detach().real, cmap=color_theme), ax=axs[1, 0])\n",
    "\n",
    "axs[1, 1].imshow(diff[0, 0].detach().real, cmap=color_theme)\n",
    "axs[1, 1].set_title(\"Error: $G_s[M(I)]-M(G_s[I])$\")\n",
    "plt.colorbar(axs[1, 1].imshow(diff[0, 0].detach().real, cmap=color_theme), ax=axs[1, 1])\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print(\"Equivariant Error:\", torch.norm(diff).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model on MNIST\n",
    "import time\n",
    "import torch.optim as optim\n",
    "train_loader = torch.utils.data.DataLoader(mnist_data, batch_size=1024, shuffle=True)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "start_time = time.time()\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "model = model.cuda().to(dtype)\n",
    "model.train()\n",
    "for epoch in range(5):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data = data.to(dtype).cuda()\n",
    "        target = target.cuda()\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        \n",
    "        loss = loss_func(output, target.to(torch.long))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1}, Loss: {running_loss / len(train_loader)}\")\n",
    "end_time = time.time()\n",
    "print(\"Training Time: \", end_time - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imge_array = []\n",
    "feature_array_0 = []\n",
    "feature_array_1 = []\n",
    "feature_array_2 = []\n",
    "\n",
    "# get an image from MNIST dataset\n",
    "# 32, 22, 15\n",
    "img, l = mnist_data.__getitem__(32)\n",
    "img = img.to(torch.float64)\n",
    "plt.imshow(img[0], cmap=color_theme)\n",
    "plt.show()\n",
    "\n",
    "img = img.cuda()\n",
    "for i in range(8, 64, 1):\n",
    "    img_scaled = resample(img[None,], (i,i),\\\n",
    "                          complex=False, skip_nyq=True)[0] # down-scaled image size 16x16 \n",
    "    \n",
    "    model_out_scaled = model.get_feature(img_scaled[None]) #output size 8x8\n",
    "\n",
    "    imge_array.append(img_scaled[0].detach().cpu().numpy())\n",
    "    feature_array_0.append(model_out_scaled[0, 0].detach().cpu().numpy())\n",
    "    feature_array_1.append(model_out_scaled[0, -1].detach().cpu().numpy())\n",
    "    feature_array_2.append(model_out_scaled[0, -2].detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making Gif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reverse the list\n",
    "imge_array.reverse()\n",
    "feature_array_0.reverse()\n",
    "feature_array_1.reverse()\n",
    "feature_array_2.reverse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import imageio\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Create a folder to store individual frames\n",
    "os.makedirs(\"frames\", exist_ok=True)\n",
    "\n",
    "filenames = []\n",
    "terget_size = 64\n",
    "\n",
    "for i, (img_, feat_0, feat_1, feat_2) in enumerate(zip(imge_array, feature_array_0, feature_array_1, feature_array_2)):\n",
    "    fig, axs = plt.subplots(1, 5, figsize=(12, 3))\n",
    "\n",
    "    # pad all to the left and top to size terget_size img_, feat_0, feat_1, feat_2\n",
    "    imag_max = img_.max() + 1 \n",
    "    img_big = np.pad(img_, ((0, terget_size - img_.shape[0]), (0, terget_size - img_.shape[1])), mode='constant', constant_values=imag_max)\n",
    "\n",
    "    \n",
    "    axs[0].imshow(img_big, cmap='gray')\n",
    "    axs[0].set_title(\"Actual Image Size\")\n",
    "    axs[0].axis('off')\n",
    "    \n",
    "    \n",
    "    axs[1].imshow(img_, cmap='gray')\n",
    "    axs[1].set_title(\"Resolution: {}\".format(img_.shape[-2:]))\n",
    "    axs[1].axis('off')\n",
    "\n",
    "    axs[2].imshow(feat_0, cmap='viridis')\n",
    "    axs[2].set_title(\"Feature Map 0\")\n",
    "    axs[2].axis('off')\n",
    "    \n",
    "    axs[3].imshow(feat_1, cmap='viridis')\n",
    "    axs[3].set_title(\"Feature Map 1\")\n",
    "    axs[3].axis('off')\n",
    "    \n",
    "    axs[4].imshow(feat_2, cmap='viridis')\n",
    "    axs[4].set_title(\"Feature Map 2\")\n",
    "    axs[4].axis('off')\n",
    "\n",
    "    fname = f\"frames/frame_{i:02d}.png\"\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(fname)\n",
    "    plt.close(fig)\n",
    "    filenames.append(fname)\n",
    "\n",
    "# Create GIF\n",
    "gif_path = \"image_feature_animation.gif\"\n",
    "with imageio.get_writer(gif_path, mode='I', duration=0.5, loop=0) as writer:\n",
    "    for filename in filenames:\n",
    "        image = imageio.imread(filename)\n",
    "        writer.append_data(image)\n",
    "\n",
    "print(f\"GIF saved as {gif_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model weights\n",
    "# torch.save(model.state_dict(), 'model_weights_base.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imageio\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "input_gif = \"/home/rahman79/Desktop/Projects/Scale_Equivarinat_Fourier_Layer/image_feature_animation_7_ours.gif\"\n",
    "output_gif = \"/home/rahman79/Desktop/Projects/Scale_Equivarinat_Fourier_Layer/image_feature_animation_7_ours_.gif\"\n",
    "\n",
    "# Read all frames from the original GIF\n",
    "frames = imageio.mimread(input_gif)\n",
    "\n",
    "# Convert all frames to the same shape and mode (e.g., RGB and same size)\n",
    "# First, get the size of the first frame\n",
    "target_shape = frames[0].shape\n",
    "\n",
    "# Standardize all frames\n",
    "standardized_frames = []\n",
    "for frame in frames:\n",
    "    pil_frame = Image.fromarray(frame).convert(\"RGB\")\n",
    "    pil_frame = pil_frame.resize((target_shape[1], target_shape[0]))\n",
    "    standardized_frames.append(np.array(pil_frame))\n",
    "\n",
    "# Save with loop=0 (infinite loop)\n",
    "imageio.mimsave(output_gif, standardized_frames, duration=0.5, loop=0)\n",
    "\n",
    "print(f\"Saved looping GIF to {output_gif}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scale Equivarinat Linear Layer <a class=\"anchor\" id=\"linear_layer\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will explore scale equivarinat linear layer. Given a singnal $f$ of lenght $N$, a linear operation on the signal is represented as $f_{out} = W \\times f$, where $W$ is $N\\times N$ matrix.\n",
    "\n",
    "In general this operation is not scale equivarinat. But following our claim on scale equivarinat operation, we can construct scale Equivarinat Linear Layer. We demote the linear layer as $L$.\n",
    "\n",
    "We have two modules ```SpectralMixer_1D``` ,and ```SpectralMixer_2D``` for signal on 1D and 2D domain respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchaudio\n",
    "from scale_eq.utils.core_utils import resample_1d \n",
    "plt.rcParams[\"figure.figsize\"] = (30,7)\n",
    "plt.rcParams.update({\n",
    "    'font.size': 16,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yesno_data = torchaudio.datasets.YESNO('./data', download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = yesno_data.__getitem__(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For visualization we will reduce the length (time duration) of the audio signals. Here, we denote the audio signal as $f$ and its' Fourier Transform a $F$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = p[0][0,6500:7000].to(torch.double)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot([i/8000 for i in range(f.shape[-1])],f, marker='o', markersize=4, linestyle='-', color='b')\n",
    "plt.xlabel('Time (s)')\n",
    "plt.ylabel('Amplitude')\n",
    "plt.title('Original Signal: $f$')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F = torch.fft.fftshift(torch.fft.fft(f))\n",
    "plt.bar([i-F.shape[-1]//2 for i in range(F.shape[-1])],torch.abs(F), color='green', width=1.0)\n",
    "plt.xlabel('Frequency')\n",
    "plt.ylabel('Amplitude')\n",
    "plt.title('Fourier Transform Magnitude: $|F|$')\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scale_eq.layers.spectral_mixer import SpectralMixer_1D, SpectralMixer_2D\n",
    "'''\n",
    "num_channels: Number of Co-domain of the input signal\n",
    "mode_size: Maximum number of Fourier models to use. Normally can be set to size of the input signal.\n",
    "mixer_band: Limits the number of neighboring Fourier models to mix with each other. Deafults to -1 implies no limit on mixing.\n",
    "'''\n",
    "s_eq_linear = SpectralMixer_1D( num_channels=1, mode_size=500, mixer_band=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    f_out = torch.fft.ifft(s_eq_linear(torch.fft.fft(f[None,None,...],\\\n",
    "                                                     norm='forward'), 500), norm='forward')   \n",
    "    f_scaled = resample_1d(f[None,None,...], 250, skip_nyq=True)\n",
    "    f_scaled_out = torch.fft.ifft(s_eq_linear(torch.fft.fft(f_scaled,\\\n",
    "                                                            norm='forward'), 250), norm='forward')\n",
    "    f_scaled_out = resample_1d(f_scaled_out, 250, skip_nyq=True)\n",
    "    scaled_f_out =  resample_1d(f_out, 250, skip_nyq=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_values = [i / 8000 for i in range(f_out.shape[-1])]\n",
    "plt.plot(time_values, f_out[0, 0].real, marker='o', markersize=4, linestyle='-', color='b')\n",
    "plt.xlabel('Time (s)')\n",
    "plt.ylabel('Amplitude')\n",
    "plt.title('Transformed Signal: $L(f)$')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Compute f_scaled\n",
    "time_values = [i / 4000 for i in range(f_scaled.shape[-1])]\n",
    "plt.plot(time_values, f_scaled[0, 0].real, marker='o', markersize=4, linestyle='-', color='g')\n",
    "plt.xlabel('Time (s)')\n",
    "plt.ylabel('Amplitude')\n",
    "plt.title('Scaled Input Signal: $G_s(f)$')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Compute f_scaled_out\n",
    "time_values = [i / 4000 for i in range(f_scaled_out.shape[-1])]\n",
    "plt.plot(time_values, f_scaled_out[0, 0].real, marker='o', markersize=4, linestyle='-', color='r')\n",
    "plt.xlabel('Time (s)')\n",
    "plt.ylabel('Amplitude')\n",
    "plt.title('Transformed Scaled Signal: $L(G_s[f])$')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Compute difference signal\n",
    "diff = (f_scaled_out[0, 0] - scaled_f_out[0, 0]).real\n",
    "time_values = [i / 4000 for i in range(diff.shape[-1])]\n",
    "plt.plot(time_values, diff, marker='o', markersize=4, linestyle='-', color='orange')\n",
    "plt.xlabel('Time (s)')\n",
    "plt.ylabel('Amplitude')\n",
    "plt.title('Equivariance Error: $L(G_s[f]) - G_s[L(f)]$')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "print(\"Equivariant Error:\", torch.norm(diff).item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Together with the this scale equivarinat linear operation and activtion ```scaleEqNonlin1d```, we can also construct Fully connected Scale Equivariant network. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neuralop",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
