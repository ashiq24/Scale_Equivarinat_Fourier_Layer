{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from layers.spectral_conv import SpectralConv2dLocalized\n",
    "from layers.scalq_eq_nonlin import scaleEqNonlin, scaleEqNonlinMaxp,scaleEqNonlin1d\n",
    "from torchvision.datasets import KMNIST,MNIST\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from utils.core_utils import resample\n",
    "import torch.functional as F\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "plt.rcParams[\"figure.figsize\"] = (4,4)\n",
    "color_theme = 'Blues'\n",
    "plt.rcParams.update({\n",
    "    'font.size': 12,\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"loading_image\"></a>\n",
    "### Load some random image "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_data = MNIST('./data',\\\n",
    "                    transform=torchvision.transforms.Compose([torchvision.transforms.ToTensor(),\\\n",
    "                                torchvision.transforms.Normalize((0.1307,), (0.3081,))]), download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img,l = mnist_data.__getitem__(7)\n",
    "plt.imshow(img[0], cmap=color_theme)\n",
    "plt.xlabel(str(l))\n",
    "plt.colorbar(shrink=0.8)\n",
    "plt.show()\n",
    "img = img.to(torch.float64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We denote input image by $I$. And by $f*I$, we will denote the convolution of image $I$ with some filter $f$. And we will denote the scaling operation by $G_s[I]$. In our case, the scaling operation $G_s$ is ideal resampling operation.\n",
    "\n",
    "Now, any operation, $P$, on the image $I$ will ve scale equivarinat if it commutes with the scaling operation $G_s$, i.e. $$ G_s[P(I)] = P(G_s[I])$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scale Equivarinat Convolution <a class=\"anchor\" id=\"conv_layer\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To attain scale-equivariant convolution, we execute the operation in the Fourier domain. While understanding local features is crutial in computer vision, spectral convolution functions globally. To capture local features effectively, we present a localized Fourier filter, imposing constrain in Spectral domain which ensures appropriate localization of the spatial kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "local modes: Active size of the Filter (Filter is 0 outside this size)\n",
    "global_modes: Global size of the Filter (Max size of Training Images)\n",
    "'''\n",
    "conv = SpectralConv2dLocalized(in_channel=1, out_channel=1,global_modes=28,local_modes=3)\n",
    "filter = conv.get_filters()\n",
    "filter_fourier_coefficients = conv.get_filters_spectral()\n",
    "s_filter = conv.get_filters_spectral()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize=(9, 4))\n",
    "\n",
    "axs[0].imshow(filter[0, 0].real.detach(), cmap=color_theme)\n",
    "axs[0].set_title('Spatially Localized Filter')\n",
    "axs[0].set_xlabel('X-axis')\n",
    "axs[0].set_ylabel('Y-axis')\n",
    "\n",
    "axs[1].imshow(torch.fft.fftshift(torch.abs(s_filter[0, 0]).detach()), cmap=color_theme)\n",
    "axs[1].set_title('Magnitude of Spectral Coefficients')\n",
    "axs[1].set_xlabel('Horizontal Frequency')\n",
    "axs[1].set_ylabel('Verticle Frequency')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "        conv_out = torch.fft.ifft2(conv(torch.fft.fft2(img[None,], norm='forward')), norm='forward').real\n",
    "        img_scaled = resample(img[None,], (16,16),complex=False, skip_nyq=True)[0]\n",
    "        conv_out_scaled = torch.fft.ifft2(conv(torch.fft.fft2(img_scaled[None], norm='forward')), norm='forward').real\n",
    "        scaled_conv_out = resample(conv_out, (16,16), complex=False, skip_nyq=True)\n",
    "        diff = scaled_conv_out - conv_out_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ploting routine\n",
    "fig, axs = plt.subplots(2, 3, figsize=(16, 10))\n",
    "img_plot = axs[0, 0].imshow(img[0], cmap=color_theme)\n",
    "plt.colorbar(img_plot, ax=axs[0, 0], shrink=0.85)\n",
    "axs[0, 0].set_title(\"Original Image: $I$\")\n",
    "conv_plot = axs[0, 1].imshow(conv_out[0, 0].detach(), cmap=color_theme)\n",
    "plt.colorbar(conv_plot, ax=axs[0, 1], shrink=0.85)\n",
    "axs[0, 1].set_title(\"Convolved Image: $f*I$\")\n",
    "scon_plot= axs[0, 2].imshow(scaled_conv_out[0,0], cmap=color_theme)\n",
    "plt.colorbar(scon_plot, ax=axs[0, 2], shrink=0.85)\n",
    "axs[0, 2].set_title(\"Scaled Convolved Image: $G_s[f *I]$\")\n",
    "simg_plot = axs[1, 0].imshow(img_scaled[0].detach(), cmap=color_theme)\n",
    "plt.colorbar(simg_plot, ax=axs[1, 0], shrink=0.85)\n",
    "axs[1, 0].set_title(\"Scaled Image: $G_s[I]$\")\n",
    "conv_scaled_plot = axs[1, 1].imshow(conv_out_scaled[0,0], cmap=color_theme)\n",
    "plt.colorbar(conv_scaled_plot, ax=axs[1, 1], shrink=0.85)\n",
    "axs[1, 1].set_title(\"Convolved Scaled Image: f*G_s(I)\")\n",
    "diff_plot = axs[1, 2].imshow(diff[0, 0].detach(), cmap=color_theme)\n",
    "axs[1, 2].set_title(\"Error: G_s[f*I] - f*G_s[I]\")\n",
    "plt.colorbar(diff_plot, ax=axs[1, 2], shrink=0.85)\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "plt.show()\n",
    "print(\"Equivariant Error:\", torch.norm(diff).item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that with our spacially local spectral convolution, the equivariance error is close to 0.\n",
    "\n",
    "On the other hand, if we repeat the same process regular $3\\times3$ convolution, we will find that, the operation is not Scale Equivarinat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regular_conv = torch.nn.Conv2d(in_channels=1, out_channels=1, kernel_size=3, stride=1, padding=1).to(img.dtype)\n",
    "\n",
    "with torch.no_grad():\n",
    "        conv_out = regular_conv(img[None,])\n",
    "        img_scaled = resample(img[None,], (16,16),complex=False, skip_nyq=True)[0]\n",
    "        conv_out_scaled = regular_conv(img_scaled[None])\n",
    "        scaled_conv_out = resample(conv_out, (16,16), complex=False, skip_nyq=True)\n",
    "        diff = scaled_conv_out - conv_out_scaled\n",
    "        \n",
    "fig, axs = plt.subplots(2, 3, figsize=(16, 10))\n",
    "img_plot = axs[0, 0].imshow(img[0], cmap=color_theme)\n",
    "plt.colorbar(img_plot, ax=axs[0, 0], shrink=0.85)\n",
    "axs[0, 0].set_title(\"Original Image: $I$\")\n",
    "conv_plot = axs[0, 1].imshow(conv_out[0, 0].detach(), cmap=color_theme)\n",
    "plt.colorbar(conv_plot, ax=axs[0, 1], shrink=0.85)\n",
    "axs[0, 1].set_title(\"Convolved Image: $f*I$\")\n",
    "scon_plot= axs[0, 2].imshow(scaled_conv_out[0,0], cmap=color_theme)\n",
    "plt.colorbar(scon_plot, ax=axs[0, 2], shrink=0.85)\n",
    "axs[0, 2].set_title(\"Scaled Convolved Image: $G_s[f *I]$\")\n",
    "simg_plot = axs[1, 0].imshow(img_scaled[0].detach(), cmap=color_theme)\n",
    "plt.colorbar(simg_plot, ax=axs[1, 0], shrink=0.85)\n",
    "axs[1, 0].set_title(\"Scaled Image: $G_s[I]$\")\n",
    "conv_scaled_plot = axs[1, 1].imshow(conv_out_scaled[0,0], cmap=color_theme)\n",
    "plt.colorbar(conv_scaled_plot, ax=axs[1, 1], shrink=0.85)\n",
    "axs[1, 1].set_title(\"Convolved Scaled Image: f*G_s(I)\")\n",
    "diff_plot = axs[1, 2].imshow(diff[0, 0].detach(), cmap=color_theme)\n",
    "axs[1, 2].set_title(\"Error: G_s[f*I] - f*G_s[I]\")\n",
    "plt.colorbar(diff_plot, ax=axs[1, 2], shrink=0.85)\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "plt.show()\n",
    "print(\"Equivariant Error:\", torch.norm(diff).item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scale Equivarinat Non-Linearity <a class=\"anchor\" id=\"non_linearity\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets test the propsoed scale equivarinat non-linearity. Even though we are using ReLu, we can pass any activation function of our choice to the ```scaleEqNonlin``` class and can obtain a scale-equivarinat version of that non-linear activation.\n",
    "\n",
    "Here, we will demosntrate Scale Equivarinat sigmoid function denated as $\\sigma_e$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "max_res: Max scale of the image in consideration ( For example max resolution in dataset)\n",
    "base_res: Lowest scale of the image in consideration\n",
    "increment: resolutions (and corrsponding scales) to skip. increment = 1 means no scales will be skipped.\n",
    "increment = 2, will skip every other scale. Layers will not be equivarinat to those scales.\n",
    "'''\n",
    "\n",
    "non_lin_of_choice = torch.sigmoid # any non-linearity will work\n",
    "Sc_eq_activation = scaleEqNonlin(non_lin_of_choice, base_res=8, normalization=None,\\\n",
    "                                 max_res=28, increment=1,channels=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    non_lin = torch.fft.ifft2(Sc_eq_activation(torch.fft.fft2(img[None,...],norm='forward')),norm='forward').real\n",
    "    img_scaled = resample(img[None,], (16,16),complex=False, skip_nyq=True)[0]\n",
    "    non_lin_scaled = torch.fft.ifft2(Sc_eq_activation(torch.fft.fft2(img_scaled[None],norm='forward')),norm='forward').real\n",
    "    #excluding the Nyquest Frequency as FFT can not determine it's complex part for real data\n",
    "    non_lin_scaled = resample(non_lin_scaled, (16,16),complex=False, skip_nyq=True)\n",
    "    \n",
    "    scaled_non_lin = resample(non_lin, (16,16),complex=False, skip_nyq=True)\n",
    "    diff = scaled_non_lin - non_lin_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, 2, figsize=(12, 8))\n",
    "axs[0, 0].imshow(non_lin[0, 0].detach().real, cmap=color_theme)\n",
    "axs[0, 0].set_title(\"$\\sigma_e(I)$\")\n",
    "plt.colorbar(axs[0, 0].imshow(non_lin[0, 0].detach().real, cmap=color_theme), ax=axs[0, 0])\n",
    "axs[0, 1].imshow(non_lin_scaled[0, 0].detach().real, cmap=color_theme)\n",
    "axs[0, 1].set_title(\"$\\sigma_e(G_s[I])$\")\n",
    "plt.colorbar(axs[0, 1].imshow(non_lin_scaled[0, 0].detach().real, cmap=color_theme), ax=axs[0, 1])\n",
    "axs[1, 0].imshow(scaled_non_lin[0, 0].detach().real, cmap=color_theme)\n",
    "axs[1, 0].set_title(\"$G_s[\\sigma_e(I)]$\")\n",
    "plt.colorbar(axs[1, 0].imshow(scaled_non_lin[0, 0].detach().real, cmap=color_theme), ax=axs[1, 0])\n",
    "axs[1, 1].imshow(diff[0, 0].detach().real, cmap=color_theme)\n",
    "axs[1, 1].set_title(\"Error: $G_s[\\sigma_e(I)]-\\sigma_e(G_s[I])$\")\n",
    "plt.colorbar(axs[1, 1].imshow(diff[0, 0].detach().real, cmap=color_theme), ax=axs[1, 1])\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print(\"Equivariant Error:\", torch.norm(diff).item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, lets observe the situation with regular Sigmoid function applied pointwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_lin = non_lin_of_choice(img[None,...])\n",
    "img_scaled = resample(img[None,], (16,16),complex=False, skip_nyq=True)[0]\n",
    "non_lin_scaled = non_lin_of_choice(img_scaled[None])\n",
    "#excluding the Nyquest Frequency as FFT can not determine it's complex part for real data\n",
    "non_lin_scaled = resample(non_lin_scaled, (16,16),complex=False, skip_nyq=True)    \n",
    "scaled_non_lin = resample(non_lin, (16,16),complex=False, skip_nyq=True)\n",
    "diff = scaled_non_lin - non_lin_scaled\n",
    "\n",
    "fig, axs = plt.subplots(2, 2, figsize=(12, 8))\n",
    "axs[0, 0].imshow(non_lin[0, 0].detach().real, cmap=color_theme)\n",
    "axs[0, 0].set_title(\"$\\sigma(I)$\")\n",
    "plt.colorbar(axs[0, 0].imshow(non_lin[0, 0].detach().real, cmap=color_theme), ax=axs[0, 0])\n",
    "axs[0, 1].imshow(non_lin_scaled[0, 0].detach().real, cmap=color_theme)\n",
    "axs[0, 1].set_title(\"$\\sigma(G[I])$\")\n",
    "plt.colorbar(axs[0, 1].imshow(non_lin_scaled[0, 0].detach().real, cmap=color_theme), ax=axs[0, 1])\n",
    "axs[1, 0].imshow(scaled_non_lin[0, 0].detach().real, cmap=color_theme)\n",
    "axs[1, 0].set_title(\"$G_s[\\sigma(I)]$\")\n",
    "plt.colorbar(axs[1, 0].imshow(scaled_non_lin[0, 0].detach().real, cmap=color_theme), ax=axs[1, 0])\n",
    "axs[1, 1].imshow(diff[0, 0].detach().real, cmap=color_theme)\n",
    "axs[1, 1].set_title(\"Error: $G_s[\\sigma(I)]-\\sigma(G_s[I])$\")\n",
    "plt.colorbar(axs[1, 1].imshow(diff[0, 0].detach().real, cmap=color_theme), ax=axs[1, 1])\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print(\"Equivariant Error:\", torch.norm(diff).item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scale Equivarinat CNNs <a class=\"anchor\" id=\"CNN\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will create a multi-layered CNN with scale equivarinat layers. And we will show the whole achitecture is Scale Equivariant.\n",
    "\n",
    "We denote the features extracted by the CNN  as $M(I)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaleEqModel(nn.Module):\n",
    "    def __init__(self,\n",
    "                 scale_eq = True):\n",
    "        super(ScaleEqModel, self).__init__()\n",
    "        self.scale_eq = scale_eq\n",
    "        if scale_eq:\n",
    "            self.conv1 = SpectralConv2dLocalized(in_channel=1, out_channel=16,\\\n",
    "                                                global_modes=28,local_modes=5)\n",
    "            self.non_linear1 = scaleEqNonlinMaxp(nn.functional.sigmoid, 8,\\\n",
    "                                                normalization = None, channels = 16,\\\n",
    "                                                pool_window = 2, max_res = 28, increment = 1)\n",
    "\n",
    "            # note that for the following layers, we use the global_modes = 14, which half of the previous layers\n",
    "            # this is because of the max_pool by window size of 2, which scales down by 2. (28 -> 14). \n",
    "            # Same Goes for the base base_res(8->4)\n",
    "            self.conv2 = SpectralConv2dLocalized(in_channel=16, out_channel=32,\\\n",
    "                                                global_modes=14,local_modes=5)\n",
    "\n",
    "            #Not doing max pool in the last layer\n",
    "            self.non_linear2 = scaleEqNonlin(nn.functional.sigmoid, 4,\\\n",
    "                                            normalization = None, channels = 32,\\\n",
    "                                            max_res = 14, increment = 1)\n",
    "            \n",
    "            self.linear = nn.Linear(32*14*14, 10)\n",
    "        else:\n",
    "            self.conv1 = nn.Conv2d(1, 16, kernel_size=11, stride=1, padding=5)\n",
    "            self.non_linear1 = nn.Sequential(\n",
    "                nn.ReLU(),\n",
    "                nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "            )\n",
    "            self.conv2 = nn.Conv2d(16, 32, kernel_size=11, stride=1, padding=5)\n",
    "            self.non_linear2 = nn.ReLU()\n",
    "            self.linear = nn.Linear(32*14*14, 10)\n",
    "    \n",
    "    def get_feature(self, x):\n",
    "        if self.scale_eq:\n",
    "            x_ft = torch.fft.fft2(x, norm = 'forward')\n",
    "        else:\n",
    "            x_ft = x\n",
    "        c1 = self.conv1(x_ft)\n",
    "        n1 = self.non_linear1(c1)\n",
    "        c2 = self.conv2(n1)\n",
    "        n2 = self.non_linear2(c2)\n",
    "        if self.scale_eq:\n",
    "            features = torch.fft.ifft2(n2, norm = 'forward').real\n",
    "        else:\n",
    "            features = n2\n",
    "        return features\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.get_feature(x)\n",
    "        # reshape features to a fixed size\n",
    "        features_fixed_size = resample(features, (14,14))\n",
    "        if not self.scale_eq:\n",
    "            features_fixed_size = features_fixed_size.real\n",
    "        logits = self.linear(features_fixed_size.reshape(features_fixed_size.shape[0], -1))\n",
    "        return logits if not self.scale_eq else logits.real\n",
    "scale_eq = False\n",
    "dtype = torch.cdouble if scale_eq else torch.double\n",
    "model = ScaleEqModel(scale_eq=scale_eq).to(dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here the final features of out mode will be down-scaled by a factor of 2 due to the max-pooling operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = img.cpu().to(torch.double)\n",
    "with torch.no_grad():\n",
    "    model_out = model.get_feature(img[None,...]) # output shape (..., 14x14)\n",
    "    \n",
    "    img_scaled = resample(img[None,], (16,16),\\\n",
    "                          complex=False, skip_nyq=True)[0] # down-scaled image size 16x16 \n",
    "    \n",
    "    model_out_scaled = model.get_feature(img_scaled[None]) #output size 8x8\n",
    "\n",
    "    #removing the Nyquest\n",
    "    model_out_scaled = resample(model_out_scaled, (8,8),complex=False, skip_nyq=True)\n",
    "    \n",
    "    scaled_model_out = resample(model_out, (8,8),complex=False, skip_nyq=True)\n",
    "    diff = scaled_model_out - model_out_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, 2, figsize=(12, 10))\n",
    "axs[0, 0].set_title(\"Features: $M(I)$\")\n",
    "plt.colorbar(axs[0, 0].imshow(model_out[0, 0].detach().real, cmap=color_theme), ax=axs[0, 0])\n",
    "\n",
    "axs[0, 1].set_title(\"Features from Scaled Image: $M(G_s[I])$\")\n",
    "plt.colorbar(axs[0, 1].imshow(model_out_scaled[0, 0].detach().real, cmap=color_theme), ax=axs[0, 1])\n",
    "\n",
    "axs[1, 0].set_title(\"Scaled Features: $G_s[M(I)]$\")\n",
    "plt.colorbar(axs[1, 0].imshow(scaled_model_out[0, 0].detach().real, cmap=color_theme), ax=axs[1, 0])\n",
    "\n",
    "axs[1, 1].imshow(diff[0, 0].detach().real, cmap=color_theme)\n",
    "axs[1, 1].set_title(\"Error: $G_s[M(I)]-M(G_s[I])$\")\n",
    "plt.colorbar(axs[1, 1].imshow(diff[0, 0].detach().real, cmap=color_theme), ax=axs[1, 1])\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print(\"Equivariant Error:\", torch.norm(diff).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model on MNIST\n",
    "import time\n",
    "import torch.optim as optim\n",
    "train_loader = torch.utils.data.DataLoader(mnist_data, batch_size=1024, shuffle=True)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "start_time = time.time()\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "model = model.cuda().to(dtype)\n",
    "model.train()\n",
    "for epoch in range(5):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data = data.to(dtype).cuda()\n",
    "        target = target.cuda()\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        \n",
    "        loss = loss_func(output, target.to(torch.long))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1}, Loss: {running_loss / len(train_loader)}\")\n",
    "end_time = time.time()\n",
    "print(\"Training Time: \", end_time - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 400x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "imge_array = []\n",
    "feature_array_0 = []\n",
    "feature_array_1 = []\n",
    "feature_array_2 = []\n",
    "\n",
    "# get an image from MNIST dataset\n",
    "# 32, 22, 15\n",
    "img, l = mnist_data.__getitem__(32)\n",
    "img = img.to(torch.float64)\n",
    "plt.imshow(img[0], cmap=color_theme)\n",
    "plt.show()\n",
    "\n",
    "img = img.cuda()\n",
    "for i in range(8, 64, 1):\n",
    "    img_scaled = resample(img[None,], (i,i),\\\n",
    "                          complex=False, skip_nyq=True)[0] # down-scaled image size 16x16 \n",
    "    \n",
    "    model_out_scaled = model.get_feature(img_scaled[None]) #output size 8x8\n",
    "\n",
    "    imge_array.append(img_scaled[0].detach().cpu().numpy())\n",
    "    feature_array_0.append(model_out_scaled[0, 0].detach().cpu().numpy())\n",
    "    feature_array_1.append(model_out_scaled[0, -1].detach().cpu().numpy())\n",
    "    feature_array_2.append(model_out_scaled[0, -2].detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making Gif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reverse the list\n",
    "imge_array.reverse()\n",
    "feature_array_0.reverse()\n",
    "feature_array_1.reverse()\n",
    "feature_array_2.reverse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3678934/1007978586.py:51: DeprecationWarning: Starting with ImageIO v3 the behavior of this function will switch to that of iio.v3.imread. To keep the current behavior (and make this warning disappear) use `import imageio.v2 as imageio` or call `imageio.v2.imread` directly.\n",
      "  image = imageio.imread(filename)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GIF saved as image_feature_animation.gif\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import imageio\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Create a folder to store individual frames\n",
    "os.makedirs(\"frames\", exist_ok=True)\n",
    "\n",
    "filenames = []\n",
    "terget_size = 64\n",
    "\n",
    "for i, (img_, feat_0, feat_1, feat_2) in enumerate(zip(imge_array, feature_array_0, feature_array_1, feature_array_2)):\n",
    "    fig, axs = plt.subplots(1, 5, figsize=(12, 3))\n",
    "\n",
    "    # pad all to the left and top to size terget_size img_, feat_0, feat_1, feat_2\n",
    "    imag_max = img_.max() + 1 \n",
    "    img_big = np.pad(img_, ((0, terget_size - img_.shape[0]), (0, terget_size - img_.shape[1])), mode='constant', constant_values=imag_max)\n",
    "\n",
    "    \n",
    "    axs[0].imshow(img_big, cmap='gray')\n",
    "    axs[0].set_title(\"Actual Image Size\")\n",
    "    axs[0].axis('off')\n",
    "    \n",
    "    \n",
    "    axs[1].imshow(img_, cmap='gray')\n",
    "    axs[1].set_title(\"Resolution: {}\".format(img_.shape[-2:]))\n",
    "    axs[1].axis('off')\n",
    "\n",
    "    axs[2].imshow(feat_0, cmap='viridis')\n",
    "    axs[2].set_title(\"Feature Map 0\")\n",
    "    axs[2].axis('off')\n",
    "    \n",
    "    axs[3].imshow(feat_1, cmap='viridis')\n",
    "    axs[3].set_title(\"Feature Map 1\")\n",
    "    axs[3].axis('off')\n",
    "    \n",
    "    axs[4].imshow(feat_2, cmap='viridis')\n",
    "    axs[4].set_title(\"Feature Map 2\")\n",
    "    axs[4].axis('off')\n",
    "\n",
    "    fname = f\"frames/frame_{i:02d}.png\"\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(fname)\n",
    "    plt.close(fig)\n",
    "    filenames.append(fname)\n",
    "\n",
    "# Create GIF\n",
    "gif_path = \"image_feature_animation.gif\"\n",
    "with imageio.get_writer(gif_path, mode='I', duration=0.5, loop=0) as writer:\n",
    "    for filename in filenames:\n",
    "        image = imageio.imread(filename)\n",
    "        writer.append_data(image)\n",
    "\n",
    "print(f\"GIF saved as {gif_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model weights\n",
    "# torch.save(model.state_dict(), 'model_weights_base.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved looping GIF to /home/rahman79/Desktop/Projects/Scale_Equivarinat_Fourier_Layer/image_feature_animation_7_ours_.gif\n"
     ]
    }
   ],
   "source": [
    "import imageio\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "input_gif = \"/home/rahman79/Desktop/Projects/Scale_Equivarinat_Fourier_Layer/image_feature_animation_7_ours.gif\"\n",
    "output_gif = \"/home/rahman79/Desktop/Projects/Scale_Equivarinat_Fourier_Layer/image_feature_animation_7_ours_.gif\"\n",
    "\n",
    "# Read all frames from the original GIF\n",
    "frames = imageio.mimread(input_gif)\n",
    "\n",
    "# Convert all frames to the same shape and mode (e.g., RGB and same size)\n",
    "# First, get the size of the first frame\n",
    "target_shape = frames[0].shape\n",
    "\n",
    "# Standardize all frames\n",
    "standardized_frames = []\n",
    "for frame in frames:\n",
    "    pil_frame = Image.fromarray(frame).convert(\"RGB\")\n",
    "    pil_frame = pil_frame.resize((target_shape[1], target_shape[0]))\n",
    "    standardized_frames.append(np.array(pil_frame))\n",
    "\n",
    "# Save with loop=0 (infinite loop)\n",
    "imageio.mimsave(output_gif, standardized_frames, duration=0.5, loop=0)\n",
    "\n",
    "print(f\"Saved looping GIF to {output_gif}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scale Equivarinat Linear Layer <a class=\"anchor\" id=\"linear_layer\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will explore scale equivarinat linear layer. Given a singnal $f$ of lenght $N$, a linear operation on the signal is represented as $f_{out} = W \\times f$, where $W$ is $N\\times N$ matrix.\n",
    "\n",
    "In general this operation is not scale equivarinat. But following our claim on scale equivarinat operation, we can construct scale Equivarinat Linear Layer. We demote the linear layer as $L$.\n",
    "\n",
    "We have two modules ```SpectralMixer_1D``` ,and ```SpectralMixer_2D``` for signal on 1D and 2D domain respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchaudio\n",
    "from utils.core_utils import resample_1d \n",
    "plt.rcParams[\"figure.figsize\"] = (30,7)\n",
    "plt.rcParams.update({\n",
    "    'font.size': 16,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yesno_data = torchaudio.datasets.YESNO('./data', download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = yesno_data.__getitem__(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For visualization we will reduce the length (time duration) of the audio signals. Here, we denote the audio signal as $f$ and its' Fourier Transform a $F$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = p[0][0,6500:7000].to(torch.double)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot([i/8000 for i in range(f.shape[-1])],f, marker='o', markersize=4, linestyle='-', color='b')\n",
    "plt.xlabel('Time (s)')\n",
    "plt.ylabel('Amplitude')\n",
    "plt.title('Original Signal: $f$')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F = torch.fft.fftshift(torch.fft.fft(f))\n",
    "plt.bar([i-F.shape[-1]//2 for i in range(F.shape[-1])],torch.abs(F), color='green', width=1.0)\n",
    "plt.xlabel('Frequency')\n",
    "plt.ylabel('Amplitude')\n",
    "plt.title('Fourier Transform Magnitude: $|F|$')\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from layers.spectral_mixer import SpectralMixer_1D, SpectralMixer_2D\n",
    "'''\n",
    "num_channels: Number of Co-domain of the input signal\n",
    "mode_size: Maximum number of Fourier models to use. Normally can be set to size of the input signal.\n",
    "mixer_band: Limits the number of neighboring Fourier models to mix with each other. Deafults to -1 implies no limit on mixing.\n",
    "'''\n",
    "s_eq_linear = SpectralMixer_1D( num_channels=1, mode_size=500, mixer_band=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    f_out = torch.fft.ifft(s_eq_linear(torch.fft.fft(f[None,None,...],\\\n",
    "                                                     norm='forward'), 500), norm='forward')   \n",
    "    f_scaled = resample_1d(f[None,None,...], 250, skip_nyq=True)\n",
    "    f_scaled_out = torch.fft.ifft(s_eq_linear(torch.fft.fft(f_scaled,\\\n",
    "                                                            norm='forward'), 250), norm='forward')\n",
    "    f_scaled_out = resample_1d(f_scaled_out, 250, skip_nyq=True)\n",
    "    scaled_f_out =  resample_1d(f_out, 250, skip_nyq=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_values = [i / 8000 for i in range(f_out.shape[-1])]\n",
    "plt.plot(time_values, f_out[0, 0].real, marker='o', markersize=4, linestyle='-', color='b')\n",
    "plt.xlabel('Time (s)')\n",
    "plt.ylabel('Amplitude')\n",
    "plt.title('Transformed Signal: $L(f)$')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Compute f_scaled\n",
    "time_values = [i / 4000 for i in range(f_scaled.shape[-1])]\n",
    "plt.plot(time_values, f_scaled[0, 0].real, marker='o', markersize=4, linestyle='-', color='g')\n",
    "plt.xlabel('Time (s)')\n",
    "plt.ylabel('Amplitude')\n",
    "plt.title('Scaled Input Signal: $G_s(f)$')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Compute f_scaled_out\n",
    "time_values = [i / 4000 for i in range(f_scaled_out.shape[-1])]\n",
    "plt.plot(time_values, f_scaled_out[0, 0].real, marker='o', markersize=4, linestyle='-', color='r')\n",
    "plt.xlabel('Time (s)')\n",
    "plt.ylabel('Amplitude')\n",
    "plt.title('Transformed Scaled Signal: $L(G_s[f])$')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Compute difference signal\n",
    "diff = (f_scaled_out[0, 0] - scaled_f_out[0, 0]).real\n",
    "time_values = [i / 4000 for i in range(diff.shape[-1])]\n",
    "plt.plot(time_values, diff, marker='o', markersize=4, linestyle='-', color='orange')\n",
    "plt.xlabel('Time (s)')\n",
    "plt.ylabel('Amplitude')\n",
    "plt.title('Equivariance Error: $L(G_s[f]) - G_s[L(f)]$')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "print(\"Equivariant Error:\", torch.norm(diff).item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Together with the this scale equivarinat linear operation and activtion ```scaleEqNonlin1d```, we can also construct Fully connected Scale Equivariant network. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neuralop",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
